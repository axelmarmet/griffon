{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77110e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed(seed = 1810):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    #torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set seed for reproducibility\n",
    "SEED = 13\n",
    "seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f472c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from griffon.preprocessing import Tokenizer, CorpusLoader, Vocab\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "corpus_loader = CorpusLoader(\"../data/intermediary/corpus\", Tokenizer())\n",
    "word2vec_embedding_dim = 64\n",
    "\n",
    "model = Word2Vec(corpus_loader,\n",
    "                 vector_size = word2vec_embedding_dim,\n",
    "                 window = 3,\n",
    "                 min_count = 3,\n",
    "                 seed = SEED,\n",
    "                 workers = 1)\n",
    "\n",
    "vocab = Vocab(model.wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "forall (x y : Carrier (cart E F)) (_ : @Equal (cart E F) x y), @Equal E (proj1 x) (proj1 y) somerandomword\n",
      "['forall', 'x', 'y', 'Carrier', 'cart', 'E', 'F', 'Equal', 'cart', 'E', 'F', 'x', 'y', 'Equal', 'E', 'proj1', 'x', 'proj1', 'y', 'somerandomword']\n",
      "(22, 64)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "input_str = \"forall (x y : Carrier (cart E F)) (_ : @Equal (cart E F) x y), @Equal E (proj1 x) (proj1 y) somerandomword\"\n",
    "tokenized = tokenizer(input_str)\n",
    "print(len(tokenized))\n",
    "print(input_str)\n",
    "print(tokenized)\n",
    "print(vocab.sentence_to_tensor(tokenized).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce809349",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/vocab.pickle\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_cos_add(word_vec, init, sub, add):\n",
    "    res = word_vec.most_similar(positive = [init, add], negative = [sub], topn = 5)\n",
    "    print(f\"{init} - {sub} + {add} = {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49100345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROM DL4NLP CLASS\n",
    "\n",
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# perform PCA\n",
    "def fit_pca(model):\n",
    "    \n",
    "    # the pca_model will preserve only the first two principal components\n",
    "    pca_model = PCA(n_components = 2)\n",
    "    \n",
    "    # perform PCA on the model's word embeddings matrix\n",
    "    pca_model.fit(model.vectors)\n",
    "    \n",
    "    # return the pca_model\n",
    "    return pca_model\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# plot words\n",
    "def wordlist_2dplot_pca(model, pca_model, word_list):\n",
    "    \n",
    "    print(type(model))\n",
    "    # convert the list of words the their relative word embeddings\n",
    "    word_vecs = np.vstack([model[w] for w in word_list])\n",
    "    \n",
    "    # project the word embeddings to the 2D subspace\n",
    "    reduced_wordembs = pca_model.transform(word_vecs)\n",
    "    \n",
    "    # plot each projected word embedding\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(reduced_wordembs[:, 0], reduced_wordembs[:, 1])\n",
    "    for i, n in enumerate(word_list):\n",
    "        ax.annotate(n, (reduced_wordembs[i, 0], reduced_wordembs[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "embeddings = word_vectors.vectors\n",
    "\n",
    "embedded_embeddings = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random').fit_transform(embeddings)\n",
    "\n",
    "embedded_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from adjustText import adjust_text\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "def visualize(model, word_list:List[str]):\n",
    "    def get_tsne_embeddings(model, perplexity):\n",
    "        return TSNE(n_components=2, learning_rate='auto',\n",
    "                      init='random', perplexity=perplexity).fit_transform(model.vectors)\n",
    "\n",
    "    def wordlist_2dplot_tSNE(ax, model, sne_embeddings, word_list):\n",
    "        [model.key_to_index[w] for w in word_list]\n",
    "        # convert the list of words the their relative word embeddings\n",
    "        word_vecs = np.vstack([model.key_to_index[w] for w in word_list]).reshape((-1))\n",
    "        print(word_vecs.shape)\n",
    "        print(sne_embeddings.shape)\n",
    "        # project the word embeddings to the 2D subspace\n",
    "        reduced_wordembs = sne_embeddings[word_vecs]\n",
    "        print(reduced_wordembs.shape)\n",
    "\n",
    "        # plot each projected word embedding\n",
    "    #    fig, ax = plt.subplots()\n",
    "        ax.plot(reduced_wordembs[:,0], reduced_wordembs[:, 1], 'bo')\n",
    "        text = []\n",
    "        for i, n in enumerate(word_list):\n",
    "            text.append(ax.text(reduced_wordembs[i, 0], reduced_wordembs[i, 1], n))\n",
    "\n",
    "        adjust_text(text, ax=ax)\n",
    "    \n",
    "    PERPLEXITIES = [10, 30, 50]\n",
    "    NR_SAMPLES = 3\n",
    "    \n",
    "    f, axs = plt.subplots(NR_SAMPLES,len(PERPLEXITIES),figsize=(15,15))\n",
    "    for y in tqdm(range(NR_SAMPLES)):\n",
    "        for x, perplexity in enumerate(PERPLEXITIES):\n",
    "            if y == 0:\n",
    "                axs[y,x].title.set_text(f\"perplexity : {perplexity}\")\n",
    "            embeddings = get_tsne_embeddings(model, perplexity)\n",
    "            wordlist_2dplot_tSNE(axs[y,x], model, embeddings, word_list)\n",
    "        \n",
    "    # Set common labels\n",
    "    f.text(0.5, 0.91, 'Perplexity', ha='center', va='center', fontsize=\"xx-large\")\n",
    "    f.text(0.06, 0.5, 'Attempts', ha='center', va='center', rotation='vertical', fontsize=\"xx-large\")\n",
    "    \n",
    "word_list = [\n",
    "            #nat ops\n",
    "            \"add\", \"sub\",\n",
    "            \"mult\", \"div\",\n",
    "            \"andb\", \"orb\", \"negb\", \"eqb\",\n",
    "            #list\n",
    "             \"app\", \"cons\", \"list\", \"nat\", \"set\",\n",
    "             \"distributive\", \"commutative\", \"transitive\", \"reflexive\",\n",
    "            # relations\n",
    "             \"eq\", \"symmetric\"]        \n",
    "        \n",
    "visualize(word_vectors, word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "\n",
    "PERPLEXITIES = [50]\n",
    "NR_SAMPLES = 3\n",
    "    \n",
    "def dummy_plot(ax):\n",
    "    x, y = np.random.random((2,10))\n",
    "    ax.plot(x, y, 'bo')\n",
    "    texts = [ax.text(x[i], y[i], 'Text%s' %i) for i in range(len(x))]\n",
    "    adjustText.adjust_text(texts, ax=ax)\n",
    "\n",
    "f, axs = plt.subplots(3,3,figsize=(15,15))\n",
    "for y in tqdm(range(NR_SAMPLES)):\n",
    "    for x, perplexity in enumerate(PERPLEXITIES):\n",
    "        if y == 0:\n",
    "            axs[y,x].title.set_text(f\"perplexity : {perplexity}\")\n",
    "        dummy_plot(axs[y,x])\n",
    "# Set common labels\n",
    "f.text(0.5, 0.91, 'Perplexity', ha='center', va='center', fontsize=\"xx-large\")\n",
    "f.text(0.06, 0.5, 'Attempts', ha='center', va='center', rotation='vertical', fontsize=\"xx-large\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9717729",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_cos_add(word_vectors, init=\"plus\", sub=\"nat\", add=\"bool\")\n",
    "three_cos_add(word_vectors, init=\"mult\", sub=\"nat\", add=\"bool\")\n",
    "three_cos_add(word_vectors, init=\"add\", sub=\"0\", add=\"mult\")\n",
    "three_cos_add(word_vectors, init=\"andb\", sub=\"bool\", add=\"prop\")\n",
    "\n",
    "three_cos_add(word_vectors, init=\"0\", sub=\"add\", add=\"mult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words\n",
    "word_list = [\n",
    "            #nat ops\n",
    "            \"add\", \"sub\",\n",
    "             \"mult\", \"div\",\n",
    "            # bool ops\n",
    "            \"andb\", \"orb\", \"negb\", \"eqb\",\n",
    "            #list\n",
    "             \"app\", \"cons\", \"list\",\n",
    "            # relations\n",
    "             \"eq\", \"sym\", \"trans\"]\n",
    "\n",
    "word_vectors.vectors\n",
    "\n",
    "# perform PCA\n",
    "pca_model = fit_pca(word_vectors)\n",
    "\n",
    "# plot the list of words\n",
    "wordlist_2dplot(word_vectors, pca_model, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"vocab.pickle\", \"wb\") as f:\n",
    "    pickle.dump(word_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -a | grep pickle"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b507fde4b7ba1e52694dcc45341e9929474d1b63f43d72f76eece533313bd962"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('griffon': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
