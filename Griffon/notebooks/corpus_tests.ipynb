{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77110e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "\n",
    "from griffon.coq_dataclasses import Stage1Sample, Stage1Token\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def seed(seed = 1810):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    #torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set seed for reproducibility\n",
    "SEED = 13\n",
    "seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea28a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"test\".split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad4b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage1Iterator():\n",
    "\n",
    "    def __init__(self, stage1_root:str):\n",
    "        self.files = glob(os.path.join(stage1_root, \"**\", \"*.pickle\"), recursive=True)\n",
    "        assert len(self.files) != 0\n",
    "        \n",
    "    def generator(self):\n",
    "\n",
    "        def flatten(tokens : List[Stage1Token])->List[str]:\n",
    "            return [subtoken for token in tokens for subtoken in token.subtokens]\n",
    "\n",
    "        for file in self.files:\n",
    "            sample:Stage1Sample = pickle.load(open(file, \"rb\"))\n",
    "            for hypothesis in sample.hypotheses:\n",
    "                yield flatten(hypothesis.tokens)\n",
    "            yield flatten(sample.goal.tokens)\n",
    "            yield flatten(sample.lemma_used)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f472c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/miniconda3/envs/griffon/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 254937 words, keeping 818 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 455982 words, keeping 1112 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 737772 words, keeping 1462 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 942204 words, keeping 1734 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 1187752 words, keeping 1895 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 1418828 words, keeping 2058 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 1610339 words, keeping 2128 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 1832406 words, keeping 2287 word types\n",
      "INFO:gensim.models.word2vec:collected 2298 word types from a corpus of 1921442 raw words and 83147 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Word2Vec\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 1926 unique words (83.81201044386422%% of original 2298, drops 372)', 'datetime': '2021-12-20T17:41:26.416084', 'gensim': '4.0.1', 'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n[GCC 7.3.0]', 'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1920870 word corpus (99.97023069132453%% of original 1921442, drops 572)', 'datetime': '2021-12-20T17:41:26.417271', 'gensim': '4.0.1', 'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n[GCC 7.3.0]', 'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 2298 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 56 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 863879.8112305395 word corpus (45.0%% of prior 1920870)', 'datetime': '2021-12-20T17:41:26.441115', 'gensim': '4.0.1', 'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n[GCC 7.3.0]', 'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1926 words and 64 dimensions: 1949112 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-12-20T17:41:26.478260', 'gensim': '4.0.1', 'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n[GCC 7.3.0]', 'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 1 workers on 1926 vocabulary and 64 features, using sg=0 hs=0 sample=0.001 negative=5 window=3', 'datetime': '2021-12-20T17:41:26.478863', 'gensim': '4.0.1', 'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n[GCC 7.3.0]', 'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 14.20% examples, 117264 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 31.36% examples, 129664 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 44.49% examples, 126044 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 62.88% examples, 132879 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 78.29% examples, 130848 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 95.26% examples, 131078 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 193 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 193 jobs\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 1 : training on 1921442 raw words (863557 effective words) took 6.6s, 130131 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 14.20% examples, 124972 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 29.72% examples, 122371 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 43.78% examples, 127339 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 64.04% examples, 137444 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 82.25% examples, 137685 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 193 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 193 jobs\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 2 : training on 1921442 raw words (864254 effective words) took 6.0s, 143816 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 16.85% examples, 144934 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 32.53% examples, 148377 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 52.90% examples, 156394 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 68.35% examples, 149702 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 92.94% examples, 156768 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 193 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 193 jobs\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 3 : training on 1921442 raw words (863196 effective words) took 5.5s, 157120 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 16.85% examples, 144765 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 33.20% examples, 148632 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 54.83% examples, 160418 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 70.44% examples, 153814 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 94.11% examples, 159023 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 193 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 193 jobs\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 4 : training on 1921442 raw words (862212 effective words) took 5.4s, 161104 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 5 - PROGRESS: at 16.85% examples, 139609 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 5 - PROGRESS: at 33.51% examples, 151932 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 5 - PROGRESS: at 53.27% examples, 155699 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 5 - PROGRESS: at 69.26% examples, 151508 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:EPOCH 5 - PROGRESS: at 93.51% examples, 157304 words/s, in_qsize 0, out_qsize 0\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 193 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 193 jobs\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 5 : training on 1921442 raw words (863444 effective words) took 5.5s, 157846 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 9607210 raw words (4316663 effective words) took 29.0s, 149008 effective words/s', 'datetime': '2021-12-20T17:41:55.448812', 'gensim': '4.0.1', 'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n[GCC 7.3.0]', 'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec(vocab=1926, vector_size=64, alpha=0.025)', 'datetime': '2021-12-20T17:41:55.450235', 'gensim': '4.0.1', 'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n[GCC 7.3.0]', 'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "stage1_iterator = Stage1Iterator(\"../data/small/data/base/stage1/train\")\n",
    "\n",
    "word2vec_embedding_dim = 64\n",
    "\n",
    "model = Word2Vec(stage1_iterator,\n",
    "                 vector_size = word2vec_embedding_dim,\n",
    "                 window = 3,\n",
    "                 min_count = 3,\n",
    "                 seed = SEED,\n",
    "                 workers = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.025,\n",
      " 'batch_words': 10000,\n",
      " 'cbow_mean': 1,\n",
      " 'comment': None,\n",
      " 'compute_loss': False,\n",
      " 'corpus_count': 83147,\n",
      " 'corpus_total_words': 1921442,\n",
      " 'cum_table': array([ 121667823,  179733941,  236488238, ..., 2147430505, 2147457076,\n",
      "       2147483647], dtype=uint32),\n",
      " 'effective_min_count': 3,\n",
      " 'epochs': 5,\n",
      " 'hashfxn': <built-in function hash>,\n",
      " 'hs': 0,\n",
      " 'layer1_size': 64,\n",
      " 'lifecycle_events': [{'datetime': '2021-12-20T17:41:26.416084',\n",
      "                       'event': 'prepare_vocab',\n",
      "                       'gensim': '4.0.1',\n",
      "                       'msg': 'effective_min_count=3 retains 1926 unique words '\n",
      "                              '(83.81201044386422%% of original 2298, drops '\n",
      "                              '372)',\n",
      "                       'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid',\n",
      "                       'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n'\n",
      "                                 '[GCC 7.3.0]'},\n",
      "                      {'datetime': '2021-12-20T17:41:26.417271',\n",
      "                       'event': 'prepare_vocab',\n",
      "                       'gensim': '4.0.1',\n",
      "                       'msg': 'effective_min_count=3 leaves 1920870 word '\n",
      "                              'corpus (99.97023069132453%% of original '\n",
      "                              '1921442, drops 572)',\n",
      "                       'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid',\n",
      "                       'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n'\n",
      "                                 '[GCC 7.3.0]'},\n",
      "                      {'datetime': '2021-12-20T17:41:26.441115',\n",
      "                       'event': 'prepare_vocab',\n",
      "                       'gensim': '4.0.1',\n",
      "                       'msg': 'downsampling leaves estimated 863879.8112305395 '\n",
      "                              'word corpus (45.0%% of prior 1920870)',\n",
      "                       'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid',\n",
      "                       'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n'\n",
      "                                 '[GCC 7.3.0]'},\n",
      "                      {'datetime': '2021-12-20T17:41:26.478260',\n",
      "                       'event': 'build_vocab',\n",
      "                       'gensim': '4.0.1',\n",
      "                       'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid',\n",
      "                       'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n'\n",
      "                                 '[GCC 7.3.0]',\n",
      "                       'trim_rule': 'None',\n",
      "                       'update': False},\n",
      "                      {'datetime': '2021-12-20T17:41:26.478863',\n",
      "                       'event': 'train',\n",
      "                       'gensim': '4.0.1',\n",
      "                       'msg': 'training model with 1 workers on 1926 '\n",
      "                              'vocabulary and 64 features, using sg=0 hs=0 '\n",
      "                              'sample=0.001 negative=5 window=3',\n",
      "                       'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid',\n",
      "                       'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n'\n",
      "                                 '[GCC 7.3.0]'},\n",
      "                      {'datetime': '2021-12-20T17:41:55.448812',\n",
      "                       'event': 'train',\n",
      "                       'gensim': '4.0.1',\n",
      "                       'msg': 'training on 9607210 raw words (4316663 '\n",
      "                              'effective words) took 29.0s, 149008 effective '\n",
      "                              'words/s',\n",
      "                       'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid',\n",
      "                       'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n'\n",
      "                                 '[GCC 7.3.0]'},\n",
      "                      {'datetime': '2021-12-20T17:41:55.450235',\n",
      "                       'event': 'created',\n",
      "                       'gensim': '4.0.1',\n",
      "                       'params': 'Word2Vec(vocab=1926, vector_size=64, '\n",
      "                                 'alpha=0.025)',\n",
      "                       'platform': 'Linux-5.11.0-41-generic-x86_64-with-debian-bullseye-sid',\n",
      "                       'python': '3.7.1 (default, Dec 14 2018, 19:28:38) \\n'\n",
      "                                 '[GCC 7.3.0]'}],\n",
      " 'load': <function call_on_class_only at 0x7f3a65348e18>,\n",
      " 'max_final_vocab': None,\n",
      " 'max_vocab_size': None,\n",
      " 'min_alpha': 0.0001,\n",
      " 'min_alpha_yet_reached': 0.00011659061661875927,\n",
      " 'min_count': 3,\n",
      " 'negative': 5,\n",
      " 'ns_exponent': 0.75,\n",
      " 'null_word': 0,\n",
      " 'random': RandomState(MT19937) at 0x7F3A65234888,\n",
      " 'raw_vocab': defaultdict(<class 'int'>, {}),\n",
      " 'running_training_loss': 0.0,\n",
      " 'sample': 0.001,\n",
      " 'seed': 13,\n",
      " 'sg': 0,\n",
      " 'sorted_vocab': 1,\n",
      " 'syn1neg': array([[ 0.5828344 ,  0.57781094, -0.43621176, ...,  0.02516494,\n",
      "        -0.11502448,  0.25695685],\n",
      "       [ 0.7242292 ,  0.70198673,  0.01853991, ...,  0.0028809 ,\n",
      "         0.45040622,  0.69138396],\n",
      "       [ 0.4009515 ,  0.6668795 , -0.27631387, ...,  0.1647549 ,\n",
      "         0.05794439,  0.48361698],\n",
      "       ...,\n",
      "       [ 0.15499145,  0.29048884, -0.04624699, ...,  0.04487296,\n",
      "         0.05998014,  0.19575916],\n",
      "       [ 0.08285086,  0.25238183, -0.16746795, ...,  0.01633573,\n",
      "         0.0562199 ,  0.15269212],\n",
      "       [ 0.1519267 ,  0.30909514, -0.050902  , ...,  0.01513572,\n",
      "         0.12346064,  0.16706517]], dtype=float32),\n",
      " 'total_train_time': 28.961430335024488,\n",
      " 'train_count': 1,\n",
      " 'vector_size': 64,\n",
      " 'window': 3,\n",
      " 'workers': 1,\n",
      " 'wv': <gensim.models.keyedvectors.KeyedVectors object at 0x7f3a66a55e80>}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(vars(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "forall (x y : Carrier (cart E F)) (_ : @Equal (cart E F) x y), @Equal E (proj1 x) (proj1 y) somerandomword\n",
      "['forall', 'x', 'y', 'Carrier', 'cart', 'E', 'F', 'Equal', 'cart', 'E', 'F', 'x', 'y', 'Equal', 'E', 'proj1', 'x', 'proj1', 'y', 'somerandomword']\n",
      "(22, 64)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "input_str = \"forall (x y : Carrier (cart E F)) (_ : @Equal (cart E F) x y), @Equal E (proj1 x) (proj1 y) somerandomword\"\n",
    "tokenized = tokenizer(input_str)\n",
    "print(len(tokenized))\n",
    "print(input_str)\n",
    "print(tokenized)\n",
    "print(vocab.sentence_to_tensor(tokenized).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce809349",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/vocab.pickle\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_cos_add(word_vec, init, sub, add):\n",
    "    res = word_vec.most_similar(positive = [init, add], negative = [sub], topn = 5)\n",
    "    print(f\"{init} - {sub} + {add} = {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49100345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROM DL4NLP CLASS\n",
    "\n",
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# perform PCA\n",
    "def fit_pca(model):\n",
    "    \n",
    "    # the pca_model will preserve only the first two principal components\n",
    "    pca_model = PCA(n_components = 2)\n",
    "    \n",
    "    # perform PCA on the model's word embeddings matrix\n",
    "    pca_model.fit(model.vectors)\n",
    "    \n",
    "    # return the pca_model\n",
    "    return pca_model\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# plot words\n",
    "def wordlist_2dplot_pca(model, pca_model, word_list):\n",
    "    \n",
    "    print(type(model))\n",
    "    # convert the list of words the their relative word embeddings\n",
    "    word_vecs = np.vstack([model[w] for w in word_list])\n",
    "    \n",
    "    # project the word embeddings to the 2D subspace\n",
    "    reduced_wordembs = pca_model.transform(word_vecs)\n",
    "    \n",
    "    # plot each projected word embedding\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(reduced_wordembs[:, 0], reduced_wordembs[:, 1])\n",
    "    for i, n in enumerate(word_list):\n",
    "        ax.annotate(n, (reduced_wordembs[i, 0], reduced_wordembs[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "embeddings = word_vectors.vectors\n",
    "\n",
    "embedded_embeddings = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random').fit_transform(embeddings)\n",
    "\n",
    "embedded_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from adjustText import adjust_text\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "def visualize(model, word_list:List[str]):\n",
    "    def get_tsne_embeddings(model, perplexity):\n",
    "        return TSNE(n_components=2, learning_rate='auto',\n",
    "                      init='random', perplexity=perplexity).fit_transform(model.vectors)\n",
    "\n",
    "    def wordlist_2dplot_tSNE(ax, model, sne_embeddings, word_list):\n",
    "        [model.key_to_index[w] for w in word_list]\n",
    "        # convert the list of words the their relative word embeddings\n",
    "        word_vecs = np.vstack([model.key_to_index[w] for w in word_list]).reshape((-1))\n",
    "        print(word_vecs.shape)\n",
    "        print(sne_embeddings.shape)\n",
    "        # project the word embeddings to the 2D subspace\n",
    "        reduced_wordembs = sne_embeddings[word_vecs]\n",
    "        print(reduced_wordembs.shape)\n",
    "\n",
    "        # plot each projected word embedding\n",
    "    #    fig, ax = plt.subplots()\n",
    "        ax.plot(reduced_wordembs[:,0], reduced_wordembs[:, 1], 'bo')\n",
    "        text = []\n",
    "        for i, n in enumerate(word_list):\n",
    "            text.append(ax.text(reduced_wordembs[i, 0], reduced_wordembs[i, 1], n))\n",
    "\n",
    "        adjust_text(text, ax=ax)\n",
    "    \n",
    "    PERPLEXITIES = [10, 30, 50]\n",
    "    NR_SAMPLES = 3\n",
    "    \n",
    "    f, axs = plt.subplots(NR_SAMPLES,len(PERPLEXITIES),figsize=(15,15))\n",
    "    for y in tqdm(range(NR_SAMPLES)):\n",
    "        for x, perplexity in enumerate(PERPLEXITIES):\n",
    "            if y == 0:\n",
    "                axs[y,x].title.set_text(f\"perplexity : {perplexity}\")\n",
    "            embeddings = get_tsne_embeddings(model, perplexity)\n",
    "            wordlist_2dplot_tSNE(axs[y,x], model, embeddings, word_list)\n",
    "        \n",
    "    # Set common labels\n",
    "    f.text(0.5, 0.91, 'Perplexity', ha='center', va='center', fontsize=\"xx-large\")\n",
    "    f.text(0.06, 0.5, 'Attempts', ha='center', va='center', rotation='vertical', fontsize=\"xx-large\")\n",
    "    \n",
    "word_list = [\n",
    "            #nat ops\n",
    "            \"add\", \"sub\",\n",
    "            \"mult\", \"div\",\n",
    "            \"andb\", \"orb\", \"negb\", \"eqb\",\n",
    "            #list\n",
    "             \"app\", \"cons\", \"list\", \"nat\", \"set\",\n",
    "             \"distributive\", \"commutative\", \"transitive\", \"reflexive\",\n",
    "            # relations\n",
    "             \"eq\", \"symmetric\"]        \n",
    "        \n",
    "visualize(word_vectors, word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "\n",
    "PERPLEXITIES = [50]\n",
    "NR_SAMPLES = 3\n",
    "    \n",
    "def dummy_plot(ax):\n",
    "    x, y = np.random.random((2,10))\n",
    "    ax.plot(x, y, 'bo')\n",
    "    texts = [ax.text(x[i], y[i], 'Text%s' %i) for i in range(len(x))]\n",
    "    adjustText.adjust_text(texts, ax=ax)\n",
    "\n",
    "f, axs = plt.subplots(3,3,figsize=(15,15))\n",
    "for y in tqdm(range(NR_SAMPLES)):\n",
    "    for x, perplexity in enumerate(PERPLEXITIES):\n",
    "        if y == 0:\n",
    "            axs[y,x].title.set_text(f\"perplexity : {perplexity}\")\n",
    "        dummy_plot(axs[y,x])\n",
    "# Set common labels\n",
    "f.text(0.5, 0.91, 'Perplexity', ha='center', va='center', fontsize=\"xx-large\")\n",
    "f.text(0.06, 0.5, 'Attempts', ha='center', va='center', rotation='vertical', fontsize=\"xx-large\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9717729",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_cos_add(word_vectors, init=\"plus\", sub=\"nat\", add=\"bool\")\n",
    "three_cos_add(word_vectors, init=\"mult\", sub=\"nat\", add=\"bool\")\n",
    "three_cos_add(word_vectors, init=\"add\", sub=\"0\", add=\"mult\")\n",
    "three_cos_add(word_vectors, init=\"andb\", sub=\"bool\", add=\"prop\")\n",
    "\n",
    "three_cos_add(word_vectors, init=\"0\", sub=\"add\", add=\"mult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words\n",
    "word_list = [\n",
    "            #nat ops\n",
    "            \"add\", \"sub\",\n",
    "             \"mult\", \"div\",\n",
    "            # bool ops\n",
    "            \"andb\", \"orb\", \"negb\", \"eqb\",\n",
    "            #list\n",
    "             \"app\", \"cons\", \"list\",\n",
    "            # relations\n",
    "             \"eq\", \"sym\", \"trans\"]\n",
    "\n",
    "word_vectors.vectors\n",
    "\n",
    "# perform PCA\n",
    "pca_model = fit_pca(word_vectors)\n",
    "\n",
    "# plot the list of words\n",
    "wordlist_2dplot(word_vectors, pca_model, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"vocab.pickle\", \"wb\") as f:\n",
    "    pickle.dump(word_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -a | grep pickle"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b507fde4b7ba1e52694dcc45341e9929474d1b63f43d72f76eece533313bd962"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('griffon': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
