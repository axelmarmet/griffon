{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import random, randint\n",
    "\n",
    "from typing import Dict, Tuple, List\n",
    "from torch import Tensor\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from griffon.utils import pad_list\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from griffon.coq_dataclasses import *\n",
    "from griffon.constants import *\n",
    "from griffon.dataset.count_dataset import CounTDataset\n",
    "\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CounTDataset(\"../data/processed/stage2/train\", \"../models/vocab.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '<unk>', '<mask>', '<pad>', 'Coq', 'Init', '.', '(']\n",
      "[Stage2Token(subtokens=[1], original_subtokens=['IHn']), Stage2Token(subtokens=[12], original_subtokens=['SerTop']), Stage2Token(subtokens=[52], original_subtokens=['ev']), Stage2Token(subtokens=[12], original_subtokens=['SerTop']), Stage2Token(subtokens=[51], original_subtokens=['double']), Stage2Token(subtokens=[11], original_subtokens=['n'])]\n",
      "tensor([[ 161,    3,    3,    3,    3],\n",
      "        [  12,    3,    3,    3,    3],\n",
      "        [2346,    3,    3,    3,    3],\n",
      "        [  77,    3,    3,    3,    3]])\n"
     ]
    }
   ],
   "source": [
    "print(ds.vocab.get_itos()[:8])\n",
    "\n",
    "test = pickle.load(open(\"../data/small/stage2/train/proof00000002.pickle\", \"rb\"))\n",
    "print(test.hypotheses[1].tokens)\n",
    "\n",
    "test_2 = next(iter(ds))\n",
    "print(test_2.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "sample\n",
      "original\n",
      "z SerTop OrderedEquation t\n",
      "input\n",
      "z SerTop OrderedEquation t\n",
      "-----------------\n",
      "1\n",
      "sample\n",
      "original\n",
      "y SerTop OrderedEquation t\n",
      "input\n",
      "y SerTop OrderedEquation t\n",
      "-----------------\n",
      "2\n",
      "sample\n",
      "original\n",
      "x SerTop OrderedEquation t\n",
      "input\n",
      "x SerTop OrderedEquation t\n",
      "-----------------\n",
      "3\n",
      "sample\n",
      "original\n",
      "H Coqlib_lib_compcert Plt SerTop ereg x SerTop ereg y\n",
      "input\n",
      "H Coqlib_lib_compcert Plt SerTop ereg x SerTop ereg y\n",
      "-----------------\n",
      "4\n",
      "sample\n",
      "original\n",
      "H0 Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg y SerTop ereg z\n",
      "input\n",
      "H0 Logic_Init_Coq eq Registers_backend_compcert <mask>_<mask>_<mask>_<mask>_<mask> SerTop ereg y SerTop ereg z\n",
      "-----------------\n",
      "5\n",
      "sample\n",
      "original\n",
      "H1 Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc y SerTop eloc z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc y SerTop eloc z SerTop OrderedEqKind lt SerTop ekind y SerTop ekind z\n",
      "input\n",
      "H1 Logic_Init_Coq or <mask>_<mask>_<mask>_<mask>_<mask> OrderedLoc lt SerTop eloc <mask>_<mask>_<mask>_<mask>_<mask> SerTop eloc z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc <mask>_<mask>_<mask>_<mask>_<mask> SerTop eloc z <mask>_<mask>_<mask>_<mask>_<mask> OrderedEqKind lt SerTop ekind y SerTop ekind z\n",
      "-----------------\n",
      "6\n",
      "sample\n",
      "original\n",
      "goal Logic_Init_Coq or Coqlib_lib_compcert Plt SerTop ereg x SerTop ereg z Logic_Init_Coq and Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg x SerTop ereg z Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc x SerTop eloc z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc x SerTop eloc z SerTop OrderedEqKind lt SerTop ekind x SerTop ekind z\n",
      "input\n",
      "goal Logic_Init_Coq or Coqlib_lib_compcert Plt SerTop ereg x SerTop ereg <mask>_<mask>_<mask>_<mask>_<mask> Logic_Init_Coq and Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg x SerTop ereg z Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc x SerTop <mask>_<mask>_<mask>_<mask>_<mask> z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert <mask>_<mask>_<mask>_<mask>_<mask> SerTop eloc x SerTop <mask>_<mask>_<mask>_<mask>_<mask> z SerTop <mask>_<mask>_<mask>_<mask>_<mask> lt SerTop ekind x <mask>_<mask>_<mask>_<mask>_<mask> ekind z\n",
      "-----------------\n",
      "7\n",
      "sample\n",
      "original\n",
      "z SerTop OrderedEquation t\n",
      "input\n",
      "z SerTop OrderedEquation t\n",
      "-----------------\n",
      "8\n",
      "sample\n",
      "original\n",
      "y SerTop OrderedEquation t\n",
      "input\n",
      "<mask>_<mask>_<mask>_<mask>_<mask> <mask>_<mask>_<mask>_<mask>_<mask> OrderedEquation t\n",
      "-----------------\n",
      "9\n",
      "sample\n",
      "original\n",
      "x SerTop OrderedEquation t\n",
      "input\n",
      "label_56 SerTop OrderedEquation <mask>_<mask>_<mask>_<mask>_<mask>\n",
      "-----------------\n",
      "10\n",
      "sample\n",
      "original\n",
      "H Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg x SerTop ereg y\n",
      "input\n",
      "H <mask>_<mask>_<mask>_<mask>_<mask> eq Registers_backend_compcert reg SerTop ereg x <mask>_<mask>_<mask>_<mask>_<mask> <mask>_<mask>_<mask>_<mask>_<mask> y\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "pad_idx = ds.vocab[PAD_TOKEN]\n",
    "print(pad_idx)\n",
    "\n",
    "\n",
    "def print_sample(vocab:Vocab, sample:CounTSample):\n",
    "    itos = ds.vocab.get_itos()\n",
    "    \n",
    "    def pretty_print(ids:Tensor):\n",
    "        tokens = ids.tolist()\n",
    "        print(\" \".join([\"_\".join([itos[subtoken] for subtoken in token if subtoken != pad_idx]) for token in tokens]))\n",
    "\n",
    "    print(\"sample\")\n",
    "#    print(\"super original token\")\n",
    "#    print(sample.input_ids.tolist())\n",
    "    original_tokens = torch.clone(sample.input_ids)\n",
    "    if torch.count_nonzero(sample.target_mask):\n",
    "        original_tokens[sample.target_mask] = sample.target_ids\n",
    "\n",
    "    print(\"original\")\n",
    "    pretty_print(original_tokens)\n",
    "    print(\"input\")\n",
    "    pretty_print(sample.input_ids)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "for i, sample in enumerate(ds):\n",
    "    print(i)\n",
    "    print_sample(ds.vocab, sample)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch(vocab:Vocab, batch:CounTBatch):\n",
    "    itos = ds.vocab\n",
    "\n",
    "    def pretty_print(ids:Tensor):\n",
    "        tokens = ids.tolist()\n",
    "        print(\" \".join([\"_\".join([itos[subtoken] for subtoken in token if subtoken != pad_idx]) for token in tokens]))\n",
    "\n",
    "    for b in range(batch.input_ids.shape[0]):\n",
    "        print(\"sample\")\n",
    "        original_tokens = torch.clone(batch.input_ids[b][[batch.input_padding_mask[b]]])\n",
    "        if torch.count_nonzero(batch.target_mask[b][batch.target_padding_mask[b]]):\n",
    "            original_tokens[batch.target_mask[b][batch.target_padding_mask[b]]] = batch.target_ids[b][batch.target_padding_mask[b]]\n",
    "\n",
    "        print(\"original\")\n",
    "        pretty_print(original_tokens)\n",
    "        print(\"input\")\n",
    "        pretty_print(batch.input_ids[b][[batch.input_padding_mask[b]]])\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "dl = DataLoader()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b507fde4b7ba1e52694dcc45341e9929474d1b63f43d72f76eece533313bd962"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
