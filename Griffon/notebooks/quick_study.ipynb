{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import random, randint\n",
    "\n",
    "from typing import Dict, Tuple, List\n",
    "from torch import Tensor\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from griffon.utils import pad_list\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from griffon.coq_dataclasses import *\n",
    "from griffon.constants import *\n",
    "from griffon.dataset.count_dataset import CounTDataset\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Path ../data/CounT/train does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71023/2442373349.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounTDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/CounT/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../models/vocab.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master_proj/Griffon/Griffon/src/griffon/dataset/count_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_path, split)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Path {root_path} does not exist\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Split {split} is not supported\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Path ../data/CounT/train does not exist"
     ]
    }
   ],
   "source": [
    "ds = CounTDataset(\"../data/CounT/train\", \"../models/vocab.pickle\")\n",
    "dl = DataLoader(ds, 10000, collate_fn=ds.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '<unk>', '<mask>', '<pad>', 'Coq', 'Init', '.', '(']\n",
      "[Stage2Token(subtokens=[1], original_subtokens=['IHn']), Stage2Token(subtokens=[12], original_subtokens=['SerTop']), Stage2Token(subtokens=[52], original_subtokens=['ev']), Stage2Token(subtokens=[12], original_subtokens=['SerTop']), Stage2Token(subtokens=[51], original_subtokens=['double']), Stage2Token(subtokens=[11], original_subtokens=['n'])]\n",
      "tensor([[ 161,    3,    3,    3,    3],\n",
      "        [  12,    3,    3,    3,    3],\n",
      "        [2346,    3,    3,    3,    3],\n",
      "        [  77,    3,    3,    3,    3]])\n"
     ]
    }
   ],
   "source": [
    "print(ds.vocab.get_itos()[:8])\n",
    "\n",
    "test = pickle.load(open(\"../data/small/stage2/train/proof00000002.pickle\", \"rb\"))\n",
    "print(test.hypotheses[1].tokens)\n",
    "\n",
    "test_2 = next(iter(ds))\n",
    "print(test_2.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "sample\n",
      "original\n",
      "H0 Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg y SerTop ereg z\n",
      "input\n",
      "H0 Logic_Init_Coq eq <mask>_<mask>_<mask>_<mask>_<mask> reg SerTop ereg y SerTop ereg z\n",
      "-----------------\n",
      "1\n",
      "sample\n",
      "original\n",
      "H1 Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc y SerTop eloc z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc y SerTop eloc z SerTop OrderedEqKind lt SerTop ekind y SerTop ekind z\n",
      "input\n",
      "H1 Logic_Init_Coq <mask>_<mask>_<mask>_<mask>_<mask> Locations_backend_compcert OrderedLoc lt SerTop <mask>_<mask>_<mask>_<mask>_<mask> y SerTop eloc z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop <mask>_<mask>_<mask>_<mask>_<mask> y SerTop eloc <mask>_<mask>_<mask>_<mask>_<mask> SerTop OrderedEqKind lt SerTop ekind y SerTop ekind z\n",
      "-----------------\n",
      "2\n",
      "sample\n",
      "original\n",
      "goal Logic_Init_Coq or Coqlib_lib_compcert Plt SerTop ereg x SerTop ereg z Logic_Init_Coq and Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg x SerTop ereg z Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc x SerTop eloc z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc x SerTop eloc z SerTop OrderedEqKind lt SerTop ekind x SerTop ekind z\n",
      "input\n",
      "goal Logic_Init_Coq or Coqlib_lib_compcert Plt SerTop ereg x SerTop <mask>_<mask>_<mask>_<mask>_<mask> z Logic_Init_Coq and Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg x SerTop ereg z Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc x <mask>_<mask>_<mask>_<mask>_<mask> eloc z Logic_Init_Coq and Logic_Init_Coq eq <mask>_<mask>_<mask>_<mask>_<mask> loc SerTop eloc x <mask>_<mask>_<mask>_<mask>_<mask> eloc z <mask>_<mask>_<mask>_<mask>_<mask> OrderedEqKind lt SerTop ekind <mask>_<mask>_<mask>_<mask>_<mask> SerTop ekind z\n",
      "-----------------\n",
      "3\n",
      "sample\n",
      "original\n",
      "H Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg x SerTop ereg y\n",
      "input\n",
      "H Logic_Init_Coq eq <mask>_<mask>_<mask>_<mask>_<mask> <mask>_<mask>_<mask>_<mask>_<mask> SerTop ereg label_56 SerTop ereg <mask>_<mask>_<mask>_<mask>_<mask>\n",
      "-----------------\n",
      "4\n",
      "sample\n",
      "original\n",
      "H1 Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc x SerTop eloc y Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc x SerTop eloc y SerTop OrderedEqKind lt SerTop ekind x SerTop ekind y\n",
      "input\n",
      "H1 <mask>_<mask>_<mask>_<mask>_<mask> or Locations_backend_compcert OrderedLoc lt SerTop eloc <mask>_<mask>_<mask>_<mask>_<mask> <mask>_<mask>_<mask>_<mask>_<mask> eloc y Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop <mask>_<mask>_<mask>_<mask>_<mask> x SerTop eloc y SerTop OrderedEqKind lt SerTop ekind x SerTop ekind y\n",
      "-----------------\n",
      "5\n",
      "sample\n",
      "original\n",
      "H0 Logic_Init_Coq or Coqlib_lib_compcert Plt SerTop ereg y SerTop ereg z Logic_Init_Coq and Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg y SerTop ereg z Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc y SerTop eloc z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc y SerTop eloc z SerTop OrderedEqKind lt SerTop ekind y SerTop ekind z\n",
      "input\n",
      "H0 Logic_Init_Coq or <mask>_<mask>_<mask>_<mask>_<mask> Plt SerTop <mask>_<mask>_<mask>_<mask>_<mask> y SerTop ereg z Logic_Init_Coq and Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg y <mask>_<mask>_<mask>_<mask>_<mask> ereg z Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc y SerTop eloc <mask>_<mask>_<mask>_<mask>_<mask> Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc y SerTop eloc z SerTop OrderedEqKind lt SerTop ekind <mask>_<mask>_<mask>_<mask>_<mask> SerTop ekind z\n",
      "-----------------\n",
      "6\n",
      "sample\n",
      "original\n",
      "goal Logic_Init_Coq or Coqlib_lib_compcert Plt SerTop ereg x SerTop ereg z Logic_Init_Coq and Logic_Init_Coq eq Registers_backend_compcert reg SerTop ereg x SerTop ereg z Logic_Init_Coq or Locations_backend_compcert OrderedLoc lt SerTop eloc x SerTop eloc z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc x SerTop eloc z SerTop OrderedEqKind lt SerTop ekind x SerTop ekind z\n",
      "input\n",
      "goal Logic_Init_Coq or Coqlib_lib_compcert <mask>_<mask>_<mask>_<mask>_<mask> SerTop ereg <mask>_<mask>_<mask>_<mask>_<mask> <mask>_<mask>_<mask>_<mask>_<mask> ereg z Logic_Init_Coq and <mask>_<mask>_<mask>_<mask>_<mask> eq Registers_backend_compcert <mask>_<mask>_<mask>_<mask>_<mask> SerTop ereg x SerTop Hu2_Heqk_HColMC z <mask>_<mask>_<mask>_<mask>_<mask> or Locations_backend_compcert <mask>_<mask>_<mask>_<mask>_<mask> lt SerTop eloc x SerTop eloc z Logic_Init_Coq and Logic_Init_Coq eq Locations_backend_compcert loc SerTop eloc x SerTop eloc z SerTop OrderedEqKind <mask>_<mask>_<mask>_<mask>_<mask> SerTop <mask>_<mask>_<mask>_<mask>_<mask> x SerTop ekind z\n",
      "-----------------\n",
      "7\n",
      "sample\n",
      "original\n",
      "goal Specif_Init_Coq sumbool Logic_Init_Coq eq SerTop equation_kind ekind0 ekind1 Logic_Init_Coq not Logic_Init_Coq eq SerTop equation_kind ekind0 ekind1\n",
      "input\n",
      "goal 7_xfoldi sumbool Logic_Init_Coq eq SerTop equation_kind ekind0 ekind1 Logic_Init_Coq <mask>_<mask>_<mask>_<mask>_<mask> Logic_Init_Coq eq SerTop equation_kind ekind0 H36_Hcheck\n",
      "-----------------\n",
      "8\n",
      "sample\n",
      "original\n",
      "H SerTop EqSet2 In q0 SerTop EqSet2 add q SerTop eqs2 e\n",
      "input\n",
      "H <mask>_<mask>_<mask>_<mask>_<mask> EqSet2 In q0 SerTop EqSet2 add q <mask>_<mask>_<mask>_<mask>_<mask> eqs2 e\n",
      "-----------------\n",
      "9\n",
      "sample\n",
      "original\n",
      "n Logic_Init_Coq not Logic_Init_Coq eq SerTop OrderedEquation t q q0\n",
      "input\n",
      "n Logic_Init_Coq <mask>_<mask>_<mask>_<mask>_<mask> Logic_Init_Coq eq SerTop OrderedEquation t q <mask>_<mask>_<mask>_<mask>_<mask>\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "pad_idx = ds.vocab[PAD_TOKEN]\n",
    "print(pad_idx)\n",
    "\n",
    "def print_sample(vocab:Vocab, sample:CounTSample):\n",
    "    itos = ds.vocab.get_itos()\n",
    "    \n",
    "    def pretty_print(ids:Tensor):\n",
    "        tokens = ids.tolist()\n",
    "        print(\" \".join([\"_\".join([itos[subtoken] for subtoken in token if subtoken != pad_idx]) for token in tokens]))\n",
    "\n",
    "    print(\"sample\")\n",
    "#    print(\"super original token\")\n",
    "#    print(sample.input_ids.tolist())\n",
    "    original_tokens = torch.clone(sample.input_ids)\n",
    "    if torch.count_nonzero(sample.target_mask):\n",
    "        original_tokens[sample.target_mask] = sample.target_ids\n",
    "\n",
    "    print(\"original\")\n",
    "    pretty_print(original_tokens)\n",
    "    print(\"input\")\n",
    "    pretty_print(sample.input_ids)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    print_sample(ds.vocab, ds[i])\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'griffon.coq_dataclasses.CounTInput'>\n",
      "sample\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [12] at index 0 does not match the shape of the indexed tensor [55] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71681/3106290574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mprint_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_71681/3106290574.py\u001b[0m in \u001b[0;36mprint_batch\u001b[0;34m(vocab, inp, tgt)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moriginal_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_padding_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_padding_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0moriginal_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_padding_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_padding_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [12] at index 0 does not match the shape of the indexed tensor [55] at index 0"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "def print_batch(vocab:Vocab, inp:CounTInput, tgt:CounTTarget):\n",
    "    itos = ds.vocab.get_itos()\n",
    "\n",
    "    def pretty_print(ids:Tensor):\n",
    "        tokens = ids.tolist()\n",
    "        print(\" \".join([\"_\".join([itos[subtoken] for subtoken in token if subtoken != pad_idx]) for token in tokens]))\n",
    "\n",
    "    for b in range(inp.input_ids.shape[0]):\n",
    "        print(\"sample\")\n",
    "        original_tokens = torch.clone(inp.input_ids[b][[inp.input_padding_mask[b]]])\n",
    "        if torch.count_nonzero(tgt.target_mask[b][tgt.target_padding_mask[b]]):\n",
    "            original_tokens[tgt.target_mask[b][tgt.target_padding_mask[b]]] = tgt.target_ids[b][tgt.target_padding_mask[b]]\n",
    "\n",
    "        print(\"original\")\n",
    "        pretty_print(original_tokens)\n",
    "        print(\"input\")\n",
    "        pretty_print(inp.input_ids[b][[inp.input_padding_mask[b]]])\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "dl = DataLoader(ds, 10, collate_fn=ds.collate_fn)\n",
    "for inp, tgt in dl:\n",
    "    print(type(inp))\n",
    "    print_batch(ds.vocab, inp, tgt)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b507fde4b7ba1e52694dcc45341e9929474d1b63f43d72f76eece533313bd962"
  },
  "kernelspec": {
   "display_name": "griffon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
