{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 9,
>>>>>>> 5ca47285774301f547fb0976b157e56a7641d04a
   "id": "77110e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
<<<<<<< HEAD
    "import math\n",
=======
    "from tqdm import tqdm\n",
>>>>>>> 5ca47285774301f547fb0976b157e56a7641d04a
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
<<<<<<< HEAD
    "from griffon.constants import TGT_IGNORE_INDEX\n",
    "\n",
    "from griffon.coq_dataclasses import Stage2Sample, Stage2Statement, Stage1Sample\n",
=======
    "from griffon.coq_dataclasses import Stage1Token, Stage1Statement, Stage1Sample\n",
>>>>>>> 5ca47285774301f547fb0976b157e56a7641d04a
    "from griffon.dataset.ct_coq_dataset import CTCoqDataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 10,
>>>>>>> 5ca47285774301f547fb0976b157e56a7641d04a
   "id": "edd36de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True],\n",
       "         [True, True]],\n",
       "\n",
       "        [[True, True],\n",
       "         [True, True]],\n",
       "\n",
       "        [[True, True],\n",
       "         [True, True]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "\n",
    "        \n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n"
=======
    "test_files = glob(\"../data/processed/stage1/test/**/*.pickle\", recursive = True)\n",
    "valid_files = glob(\"../data/processed/stage1/valid/**/*.pickle\", recursive = True)\n",
    "\n",
    "vocab = pickle.load(open(\"../models/vocab.pickle\", \"rb\"))\n",
    "\n",
    "files = test_files + valid_files"
>>>>>>> 5ca47285774301f547fb0976b157e56a7641d04a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5a4e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42714/42714 [01:16<00:00, 554.95it/s]\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "covered_words = 0\n",
    "\n",
    "def process_tokens(tokens:List[Stage1Token]):\n",
    "    words = 0\n",
    "    known_words = 0\n",
    "    for token in tokens:\n",
    "        for subtoken in token.subtokens:\n",
    "            words += 1\n",
    "            if subtoken in vocab:\n",
    "                known_words += 1\n",
    "    return words, known_words\n",
    "\n",
    "for file in tqdm(files):\n",
    "    sample:Stage1Sample = pickle.load(open(file, \"rb\"))\n",
    "    for hypothesis in sample.hypotheses:\n",
    "        tw, cw = process_tokens(hypothesis.tokens)\n",
    "        total_words += tw\n",
    "        covered_words += cw\n",
    "    tw, cw = process_tokens(sample.goal.tokens)\n",
    "    total_words += tw\n",
    "    covered_words += cw\n",
    "    tw, cw = process_tokens(sample.lemma_used)\n",
    "    total_words += tw\n",
    "    covered_words += cw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71bd227d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778208040416655"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covered_words / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763e0fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.951171875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def total_size(t:Tensor):\n",
    "    return t.element_size() * t.nelement()\n",
    "\n",
    "bytes = total_size(sample.sequences) + \\\n",
    "        total_size(sample.extended_vocabulary_ids) + \\\n",
    "        total_size(sample.pointer_pad_mask) + \\\n",
    "        total_size(sample.distances_index) + \\\n",
    "        total_size(sample.distances_bins) + \\\n",
    "        total_size(sample.padding_mask) + \\\n",
    "        total_size(sample.lemma)\n",
    "\n",
    "kb = bytes / 1024\n",
    "kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1587263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTCoqSample(hypotheses=[CTCoqStatement(tokens=tensor([[7, 1, 1, 1, 1],\n",
       "        [5, 4, 3, 1, 1],\n",
       "        [6, 1, 1, 1, 1],\n",
       "        [2, 1, 1, 1, 1]]), extended_vocabulary_ids=[7, 5, 4, 3, 6, 2], pointer_pad_mask=tensor([[ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False]]), distances=[(tensor([[ 5, 31, 29, 24],\n",
       "        [24,  1,  9, 16],\n",
       "        [29, 21,  7, 22],\n",
       "        [24, 27, 22,  6]]), tensor([1.0000e+04, 8.7975e-01, 1.0330e+00, 1.1538e+00, 1.2563e+00, 1.3411e+00,\n",
       "        1.4866e+00, 1.6302e+00, 1.7595e+00, 1.8783e+00, 1.9815e+00, 2.0893e+00,\n",
       "        2.2147e+00, 2.3878e+00, 2.4491e+00, 2.5427e+00, 2.6295e+00, 2.7803e+00,\n",
       "        2.9124e+00, 3.0180e+00, 3.1951e+00, 3.2933e+00, 3.3694e+00, 3.4717e+00,\n",
       "        3.6025e+00, 3.7403e+00, 3.9105e+00, 4.0540e+00, 4.1859e+00, 4.3461e+00,\n",
       "        4.5866e+00, 4.8835e+00]), 'ppr'), (tensor([[1, 6, 6, 5],\n",
       "        [6, 1, 3, 4],\n",
       "        [6, 3, 1, 4],\n",
       "        [5, 4, 4, 1]]), tensor([1.0000e+04, 0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
       "        5.0000e+00, 6.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00]), 'shortest_paths'), (tensor([[6, 0, 0, 0],\n",
       "        [0, 6, 0, 0],\n",
       "        [0, 0, 6, 0],\n",
       "        [0, 0, 0, 6]]), tensor([ 1.0000e+04, -5.0000e+00, -4.0000e+00, -3.0000e+00, -2.0000e+00,\n",
       "        -1.0000e+00,  0.0000e+00,  1.0000e+00,  2.0000e+00,  3.0000e+00,\n",
       "         4.0000e+00,  5.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00]), 'ancestor_sp'), (tensor([[3, 0, 0, 0],\n",
       "        [0, 3, 4, 0],\n",
       "        [0, 2, 3, 0],\n",
       "        [0, 0, 0, 3]]), tensor([ 1.0000e+04, -2.0000e+00, -1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "         2.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00]), 'sibling_sp')])], goal=CTCoqStatement(tokens=tensor([[21,  1,  1,  1,  1],\n",
       "        [14,  4,  3,  1,  1],\n",
       "        [20,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [ 5,  4,  3,  1,  1],\n",
       "        [ 6,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [15,  1,  1,  1,  1],\n",
       "        [19,  1,  1,  1,  1],\n",
       "        [ 5,  4,  3,  1,  1],\n",
       "        [ 6,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [24,  4,  3,  1,  1],\n",
       "        [25,  1,  1,  1,  1],\n",
       "        [19,  1,  1,  1,  1],\n",
       "        [ 5,  4,  3,  1,  1],\n",
       "        [ 6,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [ 9,  1,  1,  1,  1],\n",
       "        [ 5,  4,  3,  1,  1],\n",
       "        [ 6,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [12,  1,  1,  1,  1],\n",
       "        [ 7,  1,  1,  1,  1],\n",
       "        [24,  4,  3,  1,  1],\n",
       "        [25,  1,  1,  1,  1],\n",
       "        [ 5,  4,  3,  1,  1],\n",
       "        [ 6,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [ 9,  1,  1,  1,  1],\n",
       "        [ 5,  4,  3,  1,  1],\n",
       "        [ 6,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [12,  1,  1,  1,  1],\n",
       "        [ 7,  1,  1,  1,  1]]), extended_vocabulary_ids=[21, 14, 4, 3, 20, 2, 5, 4, 3, 6, 2, 15, 19, 5, 4, 3, 6, 2, 24, 4, 3, 25, 19, 5, 4, 3, 6, 2, 9, 5, 4, 3, 6, 2, 12, 7, 24, 4, 3, 25, 5, 4, 3, 6, 2, 9, 5, 4, 3, 6, 2, 12, 7], pointer_pad_mask=tensor([[ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False]]), distances=[(tensor([[ 4,  0,  0,  ...,  0,  0, 29],\n",
       "        [ 0,  2,  6,  ...,  0,  0,  0],\n",
       "        [ 0, 16,  4,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0,  ...,  5, 17,  0],\n",
       "        [ 0,  0,  0,  ..., 17,  4,  0],\n",
       "        [29,  0,  0,  ...,  0,  0,  5]]), tensor([1.0000e+04, 8.7837e-01, 1.2041e+00, 1.3698e+00, 1.5040e+00, 1.7184e+00,\n",
       "        1.9236e+00, 2.0744e+00, 2.2441e+00, 2.4284e+00, 2.5384e+00, 2.6798e+00,\n",
       "        2.8198e+00, 2.9621e+00, 3.1005e+00, 3.2153e+00, 3.3216e+00, 3.4633e+00,\n",
       "        3.5759e+00, 3.7108e+00, 3.8416e+00, 3.9406e+00, 4.0263e+00, 4.1375e+00,\n",
       "        4.2536e+00, 4.3805e+00, 4.4888e+00, 4.6118e+00, 4.7048e+00, 4.8001e+00,\n",
       "        4.8921e+00, 4.9972e+00]), 'ppr'), (tensor([[ 1,  7,  7,  ...,  9,  8,  5],\n",
       "        [ 7,  1,  3,  ..., 11, 10,  7],\n",
       "        [ 7,  3,  1,  ..., 11, 10,  7],\n",
       "        ...,\n",
       "        [ 9, 11, 11,  ...,  1,  4,  7],\n",
       "        [ 8, 10, 10,  ...,  4,  1,  6],\n",
       "        [ 5,  7,  7,  ...,  7,  6,  1]]), tensor([1.0000e+04, 0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
       "        5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01,\n",
       "        1.1000e+01, 1.2000e+01, 1.3000e+01, 1.4000e+01, 1.5000e+01, 1.6000e+01,\n",
       "        1.7000e+01, 1.8000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00]), 'shortest_paths'), (tensor([[12,  0,  0,  ...,  0,  0,  0],\n",
       "        [ 0, 12,  0,  ...,  0,  0,  0],\n",
       "        [ 0,  0, 12,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0,  ..., 12,  0,  0],\n",
       "        [ 0,  0,  0,  ...,  0, 12,  0],\n",
       "        [ 0,  0,  0,  ...,  0,  0, 12]]), tensor([ 1.0000e+04, -1.1000e+01, -1.0000e+01, -9.0000e+00, -8.0000e+00,\n",
       "        -7.0000e+00, -6.0000e+00, -5.0000e+00, -4.0000e+00, -3.0000e+00,\n",
       "        -2.0000e+00, -1.0000e+00,  0.0000e+00,  1.0000e+00,  2.0000e+00,\n",
       "         3.0000e+00,  4.0000e+00,  5.0000e+00,  6.0000e+00,  7.0000e+00,\n",
       "         8.0000e+00,  9.0000e+00,  1.0000e+01,  1.1000e+01,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00]), 'ancestor_sp'), (tensor([[4, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 4, 5,  ..., 0, 0, 0],\n",
       "        [0, 3, 4,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 4, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 4, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 4]]), tensor([ 1.0000e+04, -3.0000e+00, -2.0000e+00, -1.0000e+00,  0.0000e+00,\n",
       "         1.0000e+00,  2.0000e+00,  3.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00]), 'sibling_sp')]), lemma_used=CTCoqLemma(tokens=tensor([[11,  1,  1,  1,  1],\n",
       "        [19,  1,  1,  1,  1],\n",
       "        [ 5,  4,  3,  1,  1],\n",
       "        [ 6,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [11,  1,  1,  1,  1],\n",
       "        [22,  1,  1,  1,  1],\n",
       "        [ 5,  4,  3,  1,  1],\n",
       "        [ 6,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [14,  4,  3,  1,  1],\n",
       "        [20,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [ 5,  4,  3,  1,  1],\n",
       "        [ 6,  1,  1,  1,  1],\n",
       "        [ 2,  1,  1,  1,  1],\n",
       "        [24,  4,  3,  1,  1],\n",
       "        [25,  1,  1,  1,  1],\n",
       "        [19,  1,  1,  1,  1],\n",
       "        [22,  1,  1,  1,  1],\n",
       "        [24,  4,  3,  1,  1],\n",
       "        [25,  1,  1,  1,  1],\n",
       "        [22,  1,  1,  1,  1],\n",
       "        [19,  1,  1,  1,  1]])), extended_vocabulary={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b507fde4b7ba1e52694dcc45341e9929474d1b63f43d72f76eece533313bd962"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('griffon': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
