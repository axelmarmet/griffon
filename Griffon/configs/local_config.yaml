fit:
  seed_everything: null
  trainer:
    # logger: true
    checkpoint_callback: null
    enable_checkpointing: true
    callbacks: null
    default_root_dir: null
    gradient_clip_val: null
    gradient_clip_algorithm: null
    process_position: 0
    num_nodes: 1
    num_processes: 1
    devices: null
    gpus: null
    auto_select_gpus: false
    tpu_cores: null
    ipus: null
    log_gpu_memory: null
    progress_bar_refresh_rate: null
    enable_progress_bar: true
    overfit_batches: 0.0
    track_grad_norm: -1
    check_val_every_n_epoch: 1
    fast_dev_run: false
    accumulate_grad_batches: 4
    max_epochs: 5
    min_epochs: null
    max_steps: -1
    min_steps: null
    max_time: null
    limit_train_batches: 1.0
    limit_val_batches: 1.0
    limit_test_batches: 1.0
    limit_predict_batches: 1.0
    val_check_interval: 1.0
    flush_logs_every_n_steps: null
    log_every_n_steps: 50
    accelerator: null
    strategy: null
    sync_batchnorm: false
    precision: 32
    enable_model_summary: true
    weights_summary: top
    weights_save_path: null
    num_sanity_val_steps: 2
    resume_from_checkpoint: null
    profiler: null
    benchmark: false
    deterministic: false
    reload_dataloaders_every_n_epochs: 0
    reload_dataloaders_every_epoch: false
    auto_lr_find: false
    replace_sampler_ddp: true
    detect_anomaly: false
    auto_scale_batch_size: false
    prepare_data_per_node: null
    plugins: null
    amp_backend: native
    amp_level: null
    move_metrics_to_cpu: false
    multiple_trainloader_mode: max_size_cycle
    stochastic_weight_avg: false
    terminate_on_nan: null
  model:
    config:
      architecture:
        pretrained_embeddings_path: data/small/data/base/stage2/embeddings.pkl
        vocab_file: data/small/data/base/stage2/vocab.pkl
        subtoken_embedding_dim: 128
        num_subtokens: 5
        token_embedding_dim: 256
        scale_token_embeddings: false
        activation_fn: tanh
        code_transformer:
          num_layers: 1
          encoder_layer :
            nhead: 8
            dim_feedforward: 256
            num_relative_distances: 4
          norm:
            type: layer_norm
            eps: 1.e-5
      optimizer:
        loss_fn:
          type: f_loss
          gamma: 2
        lr: 0.1
        warmup_steps: 4000

  data:
    data_root: data/small/data
    batch_size: 32
    num_workers: 4

  ckpt_path: null
